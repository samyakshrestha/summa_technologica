Metadata-Version: 2.4
Name: summa-technologica
Version: 0.1.0
Summary: CrewAI scaffold for Summa-style structured reasoning
Requires-Python: >=3.10
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: crewai>=0.114.0
Requires-Dist: python-dotenv>=1.0.0
Requires-Dist: PyYAML>=6.0
Requires-Dist: jsonschema>=4.22.0
Dynamic: license-file

# Summa Technologica

A command-line brainstorming tool that applies the medieval Scholastic method to modern questions.

You give it a question. It returns a structured argument: three objections, a counter-thesis, a central answer, and point-by-point replies. The format comes from Thomas Aquinas's *Summa Theologica*, where every question was stress-tested through formal disputation before a conclusion was reached.

Under the hood, four AI agents work in sequence. One frames the problem. One generates the strongest possible objections. One synthesizes a thesis and replies. One reviews the whole thing for logical consistency and rejects anything generic. The result is more rigorous than what a single prompt can produce, because the structure forces adversarial reasoning.

It works for any domain: physics, philosophy, economics, policy, mathematics, or anything else you want to think carefully about.

## Example

```
$ summa-technologica 'Is free will compatible with determinism?' --domain philosophy
```

Output:

```
Question: Is free will compatible with determinism?

Objections:
1. ...
2. ...
3. ...

On the contrary...
...

I answer that...
...

Replies to objections:
Reply to Objection 1. ...
Reply to Objection 2. ...
Reply to Objection 3. ...
```

## Setup

Requires Python 3.10+ and an API key from OpenAI (or another supported provider).

```bash
pip install crewai
pip install -e .
cp .env.example .env
```

Open `.env` and add your API key:

```
OPENAI_API_KEY=your-key-here
MODEL=gpt-4o-mini
```

## Usage

```bash
summa-technologica 'Your question here'
summa-technologica 'Your question here' --domain 'physics'
summa-technologica 'Your question here' --format json
summa-technologica 'Your question here' --save output.md
summa-technologica 'Your question here' --mode v2 --top 1
summa-technologica 'Your question here' --mode v2 --top 3 --format json
```

## V1 baseline benchmarking (WP1)

Run the benchmark suite and save baseline artifacts:

```bash
summa-v1-benchmark
```

Useful options:

```bash
summa-v1-benchmark --dry-run
summa-v1-benchmark --domain physics --limit 2
summa-v1-benchmark --run-label first_pass
```

Outputs are written under `eval/results/v1/<timestamp>/` with:

- `manifest.json`
- per-case JSON files
- `summary.json`
- `summary.md`

## Semantic Scholar retrieval check (WP3)

Test dual-query retrieval independently:

```bash
summa-semantic-search "Can topological invariants improve quantum error correction?" \
  --refined-query "topological order stabilizer codes anyons"
```

This prints structured JSON with:

- `status`
- `queries`
- `papers` (deduplicated across both queries)
- `errors`

## Configuration

All configuration lives in `.env`. Key options:

| Variable | Default | Purpose |
|---|---|---|
| `MODEL` | `gpt-4o-mini` | Which model to use |
| `SUMMA_VERBOSE` | `false` | Show agent reasoning in real time |
| `SUMMA_DEFAULT_DOMAIN` | `general science` | Default domain if `--domain` is not passed |
| `SEMANTIC_SCHOLAR_API_KEY` | empty | Optional API key for higher Semantic Scholar limits |
| `SEMANTIC_SCHOLAR_BASE_URL` | `https://api.semanticscholar.org` | Retrieval backend base URL |
| `SEMANTIC_SCHOLAR_TIMEOUT_SECONDS` | `20.0` | Timeout for retrieval requests |

You can use any provider supported by LiteLLM. Examples:

```
MODEL=gpt-4o-mini              # OpenAI
MODEL=deepseek/deepseek-chat   # DeepSeek (needs DEEPSEEK_API_KEY)
MODEL=gemini/gemini-2.0-flash  # Google (needs GEMINI_API_KEY)
MODEL=ollama/llama3            # Local via Ollama (free)
```

## Project structure

```
summa_technologica/
    config/
        agents.yaml        # Agent roles, goals, and backstories
        tasks.yaml         # Task prompts and sequencing
        agents_v2.yaml     # V2 six-agent roles
        tasks_v2.yaml      # V2 stage prompts
    crew.py                # CrewAI workflow definition
    crew_v2.py             # V2 six-stage pipeline with retry logic
    models.py              # Data structures and JSON validation
    v2_contracts.py        # V2 schema + validation + partial-failure contract
    cli.py                 # Command-line interface
    formatter.py           # Markdown output formatting
    formatter_v2.py        # V2 markdown rendering
    config.py              # Environment/settings loader
    semantic_scholar.py    # Semantic Scholar retrieval + citation grounding
schemas/
    hypothesis_schema.json # Canonical V2 output schema
tests/
    test_models.py         # Unit tests for the JSON parser
    test_v2_contracts.py   # Unit tests for V2 contract validation
    test_semantic_scholar.py # Unit tests for retrieval + grounding logic
```

## Customization

To change how the agents think, edit `summa_technologica/config/agents.yaml` and `summa_technologica/config/tasks.yaml`. No code changes needed.

## Author

Samyak Shrestha
